
# ğŸ  Home Credit Default Risk - Kaggle Challenge

This project is a solution to the [Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk) competition hosted on Kaggle. The goal is to predict the probability that an applicant will default on a loan based on a variety of financial, demographic, and behavioral features.

---

## ğŸ“Œ Project Overview

Home Credit seeks to expand credit opportunities to unbanked individuals. Accurate credit scoring helps them responsibly provide loans while managing risk. This project builds a predictive model that ranks applicants based on their default likelihood.

---

## ğŸ“‚ Dataset Used

The dataset contains both a main application file and multiple auxiliary tables:

- `application_train.csv` / `application_test.csv`
- `bureau.csv` / `bureau_balance.csv`
- `previous_application.csv`
- `POS_CASH_balance.csv`
- `installments_payments.csv`
- `credit_card_balance.csv`

---

## ğŸ”§ Preprocessing & Feature Engineering

- Merged and aggregated auxiliary tables into the main training set.
- Dropped columns with more than **60% missing values**.
- Imputed remaining missing values:
  - **Numerical** â†’ median
  - **Categorical** â†’ mode
- Applied **label encoding** to binary features and **one-hot encoding** to low-cardinality categorical features.
- Engineered ratio features:
  - `CREDIT_INCOME_RATIO = AMT_CREDIT / AMT_INCOME_TOTAL`
  - `ANNUITY_INCOME_RATIO = AMT_ANNUITY / AMT_INCOME_TOTAL`
  - `ANNUITY_CREDIT_RATIO = AMT_ANNUITY / AMT_CREDIT`
  - `EMPLOYED_PERCENT = DAYS_EMPLOYED / DAYS_BIRTH`

Final dataset contained **168 features** after preprocessing.

---

## âš™ï¸ Modeling Approach

- Models used:
  - Logistic Regression (baseline)
  - LightGBM (final model)
- Feature selection using **Random Forest feature importance** (top 50 features)
- Hyperparameter tuning via **Optuna**
- **5-fold stratified cross-validation** with out-of-fold (OOF) AUC evaluation
- Final model trained using best parameters from Optuna.

---

## ğŸ“ˆ Results

| Model                    | Public AUC | Private AUC |
|-------------------------|------------|-------------|
| Logistic Regression     | ~0.703     | ~0.705      |
| LightGBM (all features) | ~0.765     | 0.76718     |
| LightGBM (top 50)       | 0.76426    | **0.76966** âœ… |

---

## ğŸ“Š SHAP Explainability

- SHAP (SHapley Additive exPlanations) used to interpret model predictions.
- Highlighted top contributing features towards model decisions.

---

## ğŸ“½ï¸ Presentation Slides

You can find a detailed summary in the [ğŸ“Š PowerPoint presentation](Home_Credit_Default_Risk_Analysis.pptx).

---

## ğŸ—‚ï¸ Folder Structure

```
â”œâ”€â”€ data/                        # Raw and processed datasets
â”œâ”€â”€ notebooks/                  # Jupyter Notebooks (EDA, modeling, tuning)
â”œâ”€â”€ submission/                 # CSVs submitted to Kaggle
â”œâ”€â”€ Home_Credit_Default_Risk_Analysis.pptx  # Presentation slides
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ How to Run

1. Clone the repo:
```bash
git clone https://github.com/vigneshrb250/Home-Credit-Default-Analysis.git
cd Home-Credit-Default-Analysis
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Run the notebook `EAI Assignment.ipynb` end-to-end for training and submission generation.

---

## ğŸ™‹â€â™‚ï¸ Author

**Vignesh Ramaswamy Balasundaram**  
[LinkedIn](https://www.linkedin.com/in/vigneshrb250/) | [GitHub](https://github.com/vigneshrb250)

---

## ğŸ“Œ Final Notes

- This solution focuses on clarity, interpretability, and strong validation practices.
- Additional performance could be achieved by ensembling or using external data.

â­ If you found this helpful, consider starring the repo!
